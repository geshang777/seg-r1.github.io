<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Seg-R1: Segmentation Can Be Surprisingly Simple with Reinforcement Learning">
  <meta name="keywords" content="Foreground Segmentation, Universal Segmetation, Multi-modal Segmentation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Seg-R1: Segmentation Can Be Surprisingly Simple with Reinforcement Learning</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>




<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">
            FOCUS: Towards Universal Foreground Segmentation
          </h1>          
          <!-- <div class="publication-venue-container">
            <p class="subtitle is-4 publication-venue">AAAI 2025</p>
          </div> -->


          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a >Zuyao You</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a >Lingyu Kong</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=pUl9H8gAAAAJ&hl=zh-CN&oi=ao">Lingchen Meng</a><sup>1,2*</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=7t12hVkAAAAJ&hl=zh-CN&oi=ao">Zuxuan Wu</a><sup>1,2†</sup>,
            </span>

          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Shanghai Key Lab of Intell. Info. Processing, School of CS, Fudan University,</span>
            <span class="author-block"><sup>2</sup>Shanghai Collaborative Innovation Center of Intelligent Visual Computing</span>
            <p class="author-note">†: Corresponding author, *: These authors contributed equally.</p>
          </div>


          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2501.05238"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a "
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->

              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/geshang777/FOCUS"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              <!-- Model Link -->
              <span class="link-block">
                <a href="https://drive.google.com/drive/folders/1IcyZnqc4vcsvSUcKb2llYGPt3ClFGjPl"  
                  class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fab fa-google-drive"></i>  
                    </span>
                    <span>Model</span> 
                </a>
            </span>
            </div>
            



          </div>
        </div>
      </div>
    </div>
  </div>
</section>





<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Foreground segmentation is a fundamental task in computer vision, including various subdivision tasks.
            Previous research has typically designed task-specific architectures for foreground segmentation tasks,
            leading to a lack of unified frameworks. Moreover, they primarily focus on <span class="bold">recognizing foreground</span>
            objects without effectively <span class="bold">distinguishing the foreground from the background</span>. In this paper,
            we argue that the background and its relationship with the foreground matter. We introduce <span class="bold">FOCUS</span>,
            the <span class="bold">F</span>oreground <span class="bold">O</span>bje<span class="bold">C</span>ts <span class="bold">U</span>niversal
            <span class="bold">S</span>egmentation framework that can handle multiple foreground tasks. We develop a multi-scale
            semantic network using the edge information of objects to enhance image features. To achieve boundary-aware segmentation,
            we propose a novel distillation method, integrating the contrastive learning strategy to refine the prediction mask
            in multi-modal feature space. We conduct extensive experiments on a total of <span class="bold">13 datasets</span>
            across <span class="bold">5 tasks</span>, and the results demonstrate that FOCUS consistently outperforms the
            state-of-the-art task-specific models on most metrics.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper image and description -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Pipeline</h2>
        <div class="publication-image">
          <img src="static/images/focus_framework2.png" alt="Overview of the FOCUS framework" class="hover-zoom">
        </div>
      </div>
    </div>
    <!--/ Paper image and description -->

    <!-- Pipeline. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">

          <p>
            To universally represent the foreground and background, we borrow the object queries concept from DETR by introducing ground queries. We apply the multi-scale strategy to extract image features to feed the transformer decoder, using masked attention to enable the ground queries to focus on relevant features corresponding to foreground and background. We utilize the feature map obtained from the backbone to initialize the masked attention, which can serve as a localization prior. During this process, the ground queries adapt to learn the features relevant to the context of different tasks, making them universal features.
          </p>
          <p>
            To fully leverage the background information in images, we employ contrastive learning strategies. We propose the CLIP refiner, using the powerful multi-modal learning ability from CLIP to correct the masks generated by previous modules. We fuse the mask and image and align the fused image and its corresponding text in multi-modal feature space to refine the masks. This not only refines the edges of the mask but also accentuates the distinction between foreground and background. We treat foreground segmentation and background segmentation as two independent tasks, and in the inference stage, the probability map of both foreground and background will jointly determine the boundary of MoI.
          </p>
        </div>
      </div>
    </div>
    <!--/ Pipeline. -->


    <div class="container is-max-desktop">
      <!-- Results. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Results</h2>
        </div>
      </div>
      <!--/ Results. -->

  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <!-- 单张图片 -->
          <div class="item">
            <img src="./static/images/COD.jpg" alt="COD Image" style="width: 100%; height: auto;">
          </div>
          <div class="item">
            <img src="./static/images/SOD.jpg" alt="SOD Image" style="width: 100%; height: auto;">
          </div>
          <div class="item">
            <img src="./static/images/SD.jpg" alt="SD Image" style="width: 100%; height: auto;">
          </div>
          <div class="item">
            <img src="./static/images/DBD.jpg" alt="DBD Image" style="width: 100%; height: auto;">
          </div>
          <div class="item">
            <img src="./static/images/FD.jpg" alt="FD Image" style="width: 100%; height: auto;">
          </div>
        </div>
      </div>
    </div>
  </section>
  
  <section class="hero is-light is-small"></section>
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <!-- 单张图片 -->
          <div class="item">
            <img src="./static/images/t1.jpg" alt="t1" style="width: 100%; height: auto;">
          </div>
          <div class="item">
            <img src="./static/images/t2.jpg" alt="t2" style="width: 100%; height: auto;">
          </div>
          <div class="item">
            <img src="./static/images/t3.jpg" alt="t3" style="width: 100%; height: auto;">
        </div>
      </div>
    </div>
  </section>

  </div>
</section>








  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{you2025focus,
  title     = {{FOCUS}: Towards Universal Foreground Segmentation},
  author    = {You, Zuyao and Kong, Lingyu and Meng, Lingchen and Wu, Zuxuan},
  booktitle = {AAAI},
  year      = {2025},
}</code></pre>
  </div>
</section>

<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=f2a415&w=255&t=tt&d=7GmQevS9CR0-MfS7WwCUtrFZMS60mR35Bh0n9oaSS5I&co=ffffff&cmn=c1e0db&cmo=fdded7'></script>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <p>Template from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, thanks!</p>
    </div>
  </div>
</footer>


</body>
</html>
