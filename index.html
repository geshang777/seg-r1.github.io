<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Seg-R1: Segmentation Can Be Surprisingly Simple with Reinforcement Learning">
  <meta name="keywords" content="Foreground Segmentation, Universal Segmetation, Multi-modal Segmentation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Seg-R1: Segmentation Can Be Surprisingly Simple with Reinforcement Learning</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>




<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">
            Seg-R1: Segmentation Can Be Surprisingly Simple with Reinforcement Learning
          </h1>          
          <!-- <div class="publication-venue-container">
            <p class="subtitle is-4 publication-venue">AAAI 2025</p>
          </div> -->


          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=X8Kh8uoAAAAJ">Zuyao You</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=7t12hVkAAAAJ&hl=zh-CN&oi=ao">Zuxuan Wu</a><sup>1,2†</sup>,
            </span>

          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Shanghai Key Lab of Intell. Info. Processing, School of CS, Fudan University,</span>
            <span class="author-block"><sup>2</sup>Shanghai Collaborative Innovation Center of Intelligent Visual Computing</span>
            <p class="author-note">†: Corresponding author.</p>
          </div>


          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2501.05238"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a "
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->

              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/geshang777/Seg-R1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Demo Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/spaces/geshang/Seg-R1-demo"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-globe"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/geshang777/Seg-R1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-database"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Model Link -->
              <span class="link-block">
                <a href="https://huggingface.co/geshang/Seg-R1-7B"  
                  class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fab fa-hugging-face"></i>  
                    </span>
                    <span>Model</span> 
                </a>
            </span>
            </div>
            



          </div>
        </div>
      </div>
    </div>
  </div>
</section>





<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present <span class="bold">Seg-R1</span>, a preliminary exploration of using reinforcement learning (RL) to enhance the pixel-level understanding and reasoning capabilities of large multimodal models (LMMs). 
            Starting with foreground segmentation tasks, specifically salient object detection (SOD) and camouflaged object detection (COD), our approach enables the LMM to generate point and bounding box prompts in a next-token fashion, 
            which are then used to guide SAM2 in producing segmentation masks. We introduce <span class="bold">Group Relative Policy Optimization (GRPO)</span> into the segmentation domain, 
            equipping the LMM with pixel-level comprehension through a carefully designed training strategy. Notably, Seg-R1 achieves state-of-the-art performance with purely RL-based training, 
            achieving <span class="bold">.850</span> S-measure on COD10K without complex model modification. Moreover,  we found that pure RL training demonstrates <span class="bold">strong open-world generalization</span>. 
            Despite being trained solely on foreground segmentation data without text annotations, Seg-R1 achieves impressive zero-shot performance on referring segmentation and reasoning segmentation tasks, 
            with <span class="bold">67.3</span> cIoU on RefCOCOg val and <span class="bold">60.8</span> cIoU on ReasonSeg val, comparable with many fully supervised baselines.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper image and description -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Pipeline</h2>
        <div class="publication-image">
          <img src="static/images/teaser_2.png" alt="Overview of segmentation ability of Seg-R1." class="hover-zoom">
        </div>
      </div>
    </div>
    <!--/ Paper image and description -->

    <!-- Pipeline. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">

          <p>
            To universally represent the foreground and background, we borrow the object queries concept from DETR by introducing ground queries. We apply the multi-scale strategy to extract image features to feed the transformer decoder, using masked attention to enable the ground queries to focus on relevant features corresponding to foreground and background. We utilize the feature map obtained from the backbone to initialize the masked attention, which can serve as a localization prior. During this process, the ground queries adapt to learn the features relevant to the context of different tasks, making them universal features.
          </p>
          <p>
            To fully leverage the background information in images, we employ contrastive learning strategies. We propose the CLIP refiner, using the powerful multi-modal learning ability from CLIP to correct the masks generated by previous modules. We fuse the mask and image and align the fused image and its corresponding text in multi-modal feature space to refine the masks. This not only refines the edges of the mask but also accentuates the distinction between foreground and background. We treat foreground segmentation and background segmentation as two independent tasks, and in the inference stage, the probability map of both foreground and background will jointly determine the boundary of MoI.
          </p>
        </div>
      </div>
    </div>
    <!--/ Pipeline. -->


    <div class="container is-max-desktop">
      <!-- Results. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Results</h2>
        </div>
      </div>
      <!--/ Results. -->

  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <!-- 单张图片 -->
          <div class="item">
            <img src="./static/images/COD.jpg" alt="COD Image" style="width: 100%; height: auto;">
          </div>
          <div class="item">
            <img src="./static/images/SOD.jpg" alt="SOD Image" style="width: 100%; height: auto;">
          </div>
          <div class="item">
            <img src="./static/images/SD.jpg" alt="SD Image" style="width: 100%; height: auto;">
          </div>
          <div class="item">
            <img src="./static/images/DBD.jpg" alt="DBD Image" style="width: 100%; height: auto;">
          </div>
          <div class="item">
            <img src="./static/images/FD.jpg" alt="FD Image" style="width: 100%; height: auto;">
          </div>
        </div>
      </div>
    </div>
  </section>
  
  <section class="hero is-light is-small"></section>
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <!-- 单张图片 -->
          <div class="item">
            <img src="./static/images/t1.jpg" alt="t1" style="width: 100%; height: auto;">
          </div>
          <div class="item">
            <img src="./static/images/t2.jpg" alt="t2" style="width: 100%; height: auto;">
          </div>
          <div class="item">
            <img src="./static/images/t3.jpg" alt="t3" style="width: 100%; height: auto;">
        </div>
      </div>
    </div>
  </section>

  </div>
</section>








  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{you2025focus,
  title     = {{FOCUS}: Towards Universal Foreground Segmentation},
  author    = {You, Zuyao and Kong, Lingyu and Meng, Lingchen and Wu, Zuxuan},
  booktitle = {AAAI},
  year      = {2025},
}</code></pre>
  </div>
</section>

<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=f2a415&w=255&t=tt&d=7GmQevS9CR0-MfS7WwCUtrFZMS60mR35Bh0n9oaSS5I&co=ffffff&cmn=c1e0db&cmo=fdded7'></script>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <p>Template from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, thanks!</p>
    </div>
  </div>
</footer>


</body>
</html>
